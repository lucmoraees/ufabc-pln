{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucmoraees/ufabc-pln/blob/main/2023_Q3_PLN_ATIVIDADE_PRA%CC%81TICA_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 20/11 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnIArN0QY-Ek"
      },
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Felipe Toguchi 11202130303\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Lucas Moraes de Carvalho 11202020345\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Munir Abrão Dib 11202130880"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yExhaebs-nD"
      },
      "source": [
        "### **LIVRO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJM_qhEZRy6"
      },
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: 1`\n",
        "\n",
        "`Segundo capítulo: 25`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTwkiiGs2BV"
      },
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWsBYQNtxmum"
      },
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iHdx4BXYruQ"
      },
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw09lujGvfjc"
      },
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uo8U63jSTQ3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec783d9-46ce-401b-c751-a56bbffbcbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.3\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RyUailD5vi9E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    main_content = soup.find('main')\n",
        "\n",
        "    if main_content:\n",
        "        first_section = main_content.find('section')\n",
        "        if first_section:\n",
        "            return first_section.get_text()\n",
        "\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oTIommynRsdx"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B7SzdInoRXxW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = OPENAI_API_KEY)\n",
        "\n",
        "def chat_completion(prompt, question):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "          {\"role\": \"system\", \"content\": prompt},\n",
        "          {\"role\": \"user\", \"content\": question},\n",
        "      ],\n",
        "    temperature=0,\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iGH55svHRQks"
      },
      "outputs": [],
      "source": [
        "def correcao_gramatical(text_cap):\n",
        "  prompt = \"Atue como um especialista em gramática do português do Brasil.\"\n",
        "  question = f\"\"\"Retorne o texto abaixo com as devidas correções gramaticais:\n",
        "  \\n\\n\n",
        "  {text_cap}\n",
        "  \"\"\"\n",
        "\n",
        "  resp = chat_completion(prompt, question)\n",
        "\n",
        "  return resp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def traducao_portugues_ingles(text_cap):\n",
        "  prompt = \"Atue como um especialista em tradução de textos de português para inglês\"\n",
        "  question = f\"\"\"Traduza o texto abaixo do português para inglês:\n",
        "  \\n\\n\n",
        "  {text_cap}\n",
        "  \"\"\"\n",
        "\n",
        "  resp = chat_completion(prompt, question)\n",
        "\n",
        "  return resp"
      ],
      "metadata": {
        "id": "zQpJf0azJfhH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extracao_palavras_chave(text_cap):\n",
        "  prompt = \"Extraia as palavras-chave e retorne elas em bullet points, não repita as palavras, qundo necessário corrija a escrita do texto\"\n",
        "  question = f\"\"\"Extraia as palavras chaves do texto abaixo:\n",
        "  \\n\\n\n",
        "  {text_cap}\n",
        "  \"\"\"\n",
        "\n",
        "  resp = chat_completion(prompt, question)\n",
        "\n",
        "  return resp"
      ],
      "metadata": {
        "id": "qnV07BdKJfZW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_partes_texto(part, cap):\n",
        "    url = f\"https://brasileiraspln.com/livro-pln/1a-edicao/parte{part}/cap{cap}/cap{cap}.html\"\n",
        "    texto = get_text_from_url(url)\n",
        "\n",
        "    palavras = texto.split()\n",
        "    total_palavras = len(palavras)\n",
        "    tamanho_parte = total_palavras // 3\n",
        "\n",
        "    parte1 = ' '.join(palavras[:tamanho_parte])\n",
        "    parte2 = ' '.join(palavras[tamanho_parte:tamanho_parte*2])\n",
        "    parte3 = ' '.join(palavras[tamanho_parte*2:])\n",
        "\n",
        "    return parte1, parte2, parte3"
      ],
      "metadata": {
        "id": "U8gr8CGgCP4v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OefUW-z0P330"
      },
      "outputs": [],
      "source": [
        "def aplicar_correcao_gramatical(part, cap):\n",
        "  try:\n",
        "    parts = buscar_partes_texto(part, cap)\n",
        "\n",
        "    for part in parts:\n",
        "      if (len(part) != 0):\n",
        "        resp_1 = correcao_gramatical(part)\n",
        "        print(resp_1)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aplicar_traducao_portugues_ingles(part, cap):\n",
        "  try:\n",
        "    parts = buscar_partes_texto(part, cap)\n",
        "\n",
        "    for part in parts:\n",
        "      if (len(part) != 0):\n",
        "        resp_2 = traducao_portugues_ingles(part)\n",
        "        print(resp_2)\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")"
      ],
      "metadata": {
        "id": "RCJ3M4BiB2b6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aplicar_extracao_palavras_chave(part, cap):\n",
        "  try:\n",
        "    parts = buscar_partes_texto(part, cap)\n",
        "\n",
        "    for part in parts:\n",
        "      if (len(part) != 0):\n",
        "        resp_2 = extracao_palavras_chave(part)\n",
        "        print(resp_2)\n",
        "  except Exception as e:\n",
        "    print(f\"Erro: {e}\")"
      ],
      "metadata": {
        "id": "_gld8QTRI2jp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Capítulo 1**"
      ],
      "metadata": {
        "id": "xnEcAiMlBoGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correção gramatical"
      ],
      "metadata": {
        "id": "ClmTf322Bssx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fYFvm9S-P8P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f510f1b-4c89-45b3-b587-a40b3d812e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1 Introdução\n",
            "\n",
            "O Processamento de Linguagem Natural (PLN) é um campo de pesquisa que tem como objetivo investigar e propor métodos e sistemas de processamento computacional da linguagem humana. O adjetivo \"Natural\", na sigla, se refere às línguas faladas pelos humanos, distinguindo-as das demais linguagens (matemáticas, visuais, gestuais, de programação etc.). No decorrer deste livro, os termos \"língua\", \"linguagem humana\" e \"linguagem natural\" serão usados indistintamente; já \"linguagem\" pode eventualmente se referir a qualquer tipo de linguagem. Na área da Ciência da Computação, PLN está ligado à área de Inteligência Artificial (IA) e também está intrinsecamente relacionada à Linguística Computacional. Para deixar mais claro o que entendemos por PLN, vamos esclarecer o que se faz nessa área. De modo geral, em PLN buscam-se soluções para problemas computacionais, ou seja, tarefas, sistemas, aplicações ou programas, que requerem o tratamento computacional de uma língua (português, inglês etc.), seja escrita (texto) ou falada (fala). Línguas como as de sinais também têm sido alvo de estudos da área. Cada modo tem suas especificidades. No caso da fala, as características que a distinguem da língua escrita estão relacionadas a questões da produção (síntese) e recepção (reconhecimento) do som. Recursos da fala, como a entonação, o volume, o sotaque, podem tanto dificultar o reconhecimento ou a síntese, como também facilitar o reconhecimento de sentimentos ou intenções do falante. Qualquer que seja o modo, fala, escrita, línguas orais e línguas de sinais compartilham a dificuldade maior em PLN: a apreensão do significado de uma expressão linguística. Isso vai ficar claro no decorrer deste livro. O PLN se divide em duas grandes subáreas: Interpretação (ou Compreensão) de Linguagem Natural - NLU (do inglês, Natural Language Understanding), e Geração de Linguagem Natural - NLG (do inglês, Natural Language Generation). Situa-se em NLU tudo o que diz respeito ao processamento que visa à análise e à interpretação da língua. Por análise, entende-se a segmentação e classificação dos componentes linguísticos (por exemplo, palavras e suas classes morfológicas e gramaticais, seus traços semânticos ou ontológicos etc.). Já interpretação se refere à tentativa de apreender significados construídos pelo ser humano. Numa interação com um chatbot, por exemplo, a interpretação ocorre quando o sistema processa um texto do usuário para descobrir...\n",
            "O que ele - o sistema - deve fazer a seguir: fornecer uma resposta ou executar uma ação. Logo, ficará claro que respostas mais ou menos bem-sucedidas do sistema para o significado pretendido pelo humano podem ser suficientes para muitas aplicações, e que o completo alinhamento entre o significado pretendido pelo humano e aquele interpretado pela máquina não deve ser parte das nossas expectativas. Em NLG, por outro lado, o objetivo é a geração de linguagem natural. Um exemplo de NLG é a geração de respostas ao usuário dos chatbots. Para o sistema, isso significa decidir o que responder e como apresentar essa resposta ao usuário. Atualmente, o ChatGPT2 é o exemplo de maior sucesso: é capaz de gerar linguagem de forma tão ou mais fluente do que muitos humanos. É importante esclarecer, desde já, alguns conceitos amplamente usados ao longo deste livro. Eles dizem respeito à classificação de alguns sistemas de PLN quanto ao seu uso. Esses conceitos são: aplicações, recursos e ferramentas. Primeiramente, é relevante observar como esses conceitos se relacionam entre si. A Figura 1.1 esquematiza essa dinâmica. Figura 1.1: Relacionamento entre conceitos Como vemos na Figura 1.1, em PLN as ferramentas auxiliam na construção de uma aplicação, que pode ser um sistema computacional (desktop, web) ou um aplicativo. As aplicações fornecem um resultado ao usuário tendo uma entrada (input) ou saída (output) em linguagem natural. Aplicações fazem uso de ferramentas ou conjuntos de ferramentas, conhecidos como \"toolkits\". Também necessitam de recursos, os quais fornecem informações linguísticas necessárias para que as aplicações consigam processar a língua de maneira adequada. É importante notar que a denominação utilizada - aplicação, recurso ou ferramenta - é imprecisa e depende do uso. Por exemplo, um corretor ortográfico pode ser uma aplicação a ser usada de forma autônoma ou um passo intermediário para uma aplicação de correção de redações; um tradutor automático pode ser uma aplicação em si, com uma interface para inserir um texto de entrada e obter um texto de saída, mas também pode ser usado como ferramenta para traduzir um corpus de uma língua para outra, visando a criação de recursos em línguas de comunidades tecnologicamente menos desenvolvidas; um sumarizador automático pode ser usado para criar resumos para.\n",
            "Um usuário qualquer, mas também pode ser usado por um buscador da web como passo intermediário para um sistema de recuperação de informação; um dicionário é um recurso, mas também pode ser usado como um aplicativo para consulta; um modelo de língua pode se transformar em um chatbot, e assim por diante. Os conceitos são caracterizados e exemplificados no Quadro 1.1.\n",
            "\n",
            "Quadro 1.1 - Exemplos de aplicações, recursos e ferramentas\n",
            "\n",
            "Conceito Caracterização Exemplos\n",
            "\n",
            "Aplicações processam uma entrada (input) em linguagem natural e a transformam produzindo um determinado resultado\n",
            "- tradutor automático\n",
            "- corretor ortográfico ou gramatical\n",
            "- assistentes virtuais/chatbots\n",
            "- sumarizador automático\n",
            "- sistemas de recomendação em sites de e-commerce ou entretenimento\n",
            "- sistemas de auxílio à escrita\n",
            "- sistemas de classificação textual\n",
            "- sistemas de recuperação de informação\n",
            "- sistemas de detecção de fake news\n",
            "\n",
            "Recursos são fontes de informação linguística para sistemas\n",
            "- léxicos (listas de palavras com informações associadas) da língua em geral ou de terminologia de domínio\n",
            "- dicionários monolíngues ou bilíngues\n",
            "- corpus (datasets linguísticos) anotados manual ou automaticamente (para referência, teste ou treinamento de algoritmos de aprendizado de máquina)\n",
            "- listas de frequências de palavras\n",
            "- (mais estruturados) taxonomias, ontologias, redes de sinônimos e antônimos\n",
            "- em formato matemático: modelos de língua estatísticos (probabilidades) ou neurais (pesos) para informar qual palavra deve ser a próxima num dado contexto\n",
            "\n",
            "Ferramentas auxiliam na construção de uma aplicação ou até de outras ferramentas\n",
            "- segmentadores textuais em tokens (tokenizador), sentenças, parágrafos\n",
            "- stemmers (extratores de raiz de uma palavra (e.g. \"corr\" de \"correr\")\n",
            "- lematizadores (e.g. \"correr\" de \"corri\")\n",
            "- etiquetadores morfossintáticos (PoS taggers) para classe de palavras (verbo, substantivo, adjetivo, artigo, preposição, advérbio etc.)\n",
            "- etiquetadores semânticos, analisadores sintáticos parciais (chunkers) e completos (parsers)\n",
            "- concordanciadores\n",
            "- interfaces de anotação de corpus\n",
            "- componentes em kits de ferramentas (toolkits), como o NLTK3\n",
            "\n",
            "Neste livro, iremos aumentar gradativamente a complexidade do tratamento da língua no PLN, com foco no português brasileiro. Antes de iniciar esta trajetória, a Seção 1.2 apresenta nosso objeto de pesquisa, a língua. Em seguida, a Seção 1.3 introduz os principais paradigmas do PLN, que serão retomados em diversos momentos neste livro. Por fim, a Seção 1.4 destaca os principais pontos apresentados no capítulo.\n"
          ]
        }
      ],
      "source": [
        "aplicar_correcao_gramatical(part=1, cap=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tradução para inglês"
      ],
      "metadata": {
        "id": "4oBfdKQvBxha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaUGIbrJP_3Y",
        "outputId": "ce7b6a64-6826-4e0d-e6fe-982cfde1eff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1 Introduction Natural Language Processing (NLP) is a research field that aims to investigate and propose methods and computational systems for processing human language. The adjective \"Natural\" in the acronym refers to the languages spoken by humans, distinguishing them from other languages (mathematical, visual, gestural, programming, etc.). Throughout this book, the terms \"language,\" \"human language,\" and \"natural language\" will be used interchangeably; whereas \"language\" may occasionally refer to any type of language. In the field of Computer Science, NLP is connected to the area of Artificial Intelligence (AI) and is also intrinsically related to Computational Linguistics. To make it clearer what we mean by NLP, let's clarify what is done in this field. In general, NLP seeks solutions for computational problems, that is, tasks, systems, applications, or programs that require the computational treatment of a language (Portuguese, English, etc.), whether written (text) or spoken (speech). Sign languages have also been the subject of studies in this field. Each mode has its specificities. In the case of speech, the characteristics that distinguish it from written language are related to issues of sound production (synthesis) and reception (recognition). Speech features such as intonation, volume, accent can both hinder recognition or synthesis, as well as facilitate the recognition of speaker's emotions or intentions. Regardless of the mode, speech, writing, oral languages, and sign languages share the greatest difficulty in NLP: the apprehension of the meaning of a linguistic expression. This will become clear throughout this book. NLP is divided into two major sub-areas: Natural Language Understanding (NLU) and Natural Language Generation (NLG). NLU encompasses all processing related to the analysis and interpretation of language. Analysis refers to the segmentation and classification of linguistic components (e.g. words and their morphological and grammatical classes, their semantic or ontological features, etc.). Interpretation, on the other hand, refers to the attempt to grasp meanings constructed by humans. In an interaction with a chatbot, for example, interpretation occurs when the system processes a user's text to discover...\n",
            "what it - the system - should do next: whether to provide a response or perform an action. It will soon become clear that more or less successful responses from the system to the meaning intended by the human can be sufficient for many applications, and that complete alignment between the meaning intended by the human and that interpreted by the machine should not be part of our expectations. In NLG, on the other hand, the goal is natural language generation. An example of NLG is generating responses to user queries in chatbots. For the system, this means deciding what to respond and how to present that response to the user. Currently, ChatGPT2 is the most successful example: it is capable of generating language as fluent or even more fluent than many humans. It is important to clarify, from now on, some concepts widely used throughout this book. They refer to the classification of some NLP systems regarding their use. These concepts are: applications, resources, and tools. First, it is relevant to observe how these concepts relate to each other. Figure 1.1 outlines this dynamic. Figure 1.1: Relationship between concepts As we can see in Figure 1.1, in NLP, tools assist in the construction of an application, which can be a computer system (desktop, web) or an application. Applications provide a result to the user with a natural language input or output. Applications make use of tools or sets of tools, known as \"toolkits\". They also require resources, which provide the necessary linguistic information for applications to process language properly. It is important to note that the designation used - application, resource, or tool - is imprecise and depends on usage. For example, a spell checker can be an application to be used autonomously or an intermediate step for an essay correction application; an automatic translator can be an application itself, with an interface to input a text and obtain an output text, but it can also be used as a tool to translate a corpus from one language to another, aiming at creating resources in languages of technologically less developed communities; an automatic summarizer can be used to create summaries for\n",
            "a user can be any user, but it can also be used by a web search engine as an intermediate step for an information retrieval system; a dictionary is a resource, but it can also be used as a reference application; a language model can be transformed into a chatbot, and so on. The concepts are characterized and exemplified in Table 1.1. Table 1.1 Examples of applications, resources, and tools Concept Characterization Examples Applications process natural language input and transform it to produce a specific result - automatic translator - spell or grammar checker - virtual assistants/chatbots - automatic summarizer - recommendation systems in e-commerce or entertainment websites - writing assistance systems - text classification systems - information retrieval systems - fake news detection systems Resources are sources of linguistic information for systems - lexicons (lists of words with associated information) of the language in general or domain terminology - monolingual or bilingual dictionaries - annotated corpora (linguistic datasets) manually or automatically annotated (for reference, testing, or machine learning algorithm training) - word frequency lists - (more structured) taxonomies, ontologies, synonym and antonym networks - in mathematical format: statistical language models (probabilities) or neural models (weights) to inform which word should be the next in a given context Tools assist in the construction of an application or even other tools - text segmenters into tokens (tokenizer), sentences, paragraphs - stemmers (extractors of word roots (e.g. \"run\" from \"running\") - lemmatizers (e.g. \"run\" from \"ran\") - morphosyntactic taggers (PoS taggers) for word classes (verb, noun, adjective, article, preposition, adverb, etc.) - semantic taggers, partial syntactic analyzers (chunkers), and complete syntactic analyzers (parsers) - concordancers - corpus annotation interfaces - components in toolkits, such as NLTK3 In this book, we will gradually increase the complexity of language processing in NLP, focusing on Brazilian Portuguese. Before starting this journey, Section 1.2 presents our research object, language. Then, Section 1.3 introduces the main paradigms of NLP, which will be revisited at various points in this book. Finally, Section 1.4 highlights the main points presented in the chapter.\n"
          ]
        }
      ],
      "source": [
        "aplicar_traducao_portugues_ingles(part=1, cap=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extração das palavras chave"
      ],
      "metadata": {
        "id": "8Vx7AOXTJEv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aplicar_extracao_palavras_chave(part=1, cap=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UccVjNu3JEV9",
        "outputId": "c5474769-3c5b-4189-e7af-3cf5e907034d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Processamento de Linguagem Natural (PLN)\n",
            "- Linguagem humana\n",
            "- Línguas faladas\n",
            "- Linguagens matemáticas, visuais, gestuais, de programação\n",
            "- Ciência da Computação\n",
            "- Inteligência Artificial (IA)\n",
            "- Linguística Computacional\n",
            "- Problemas computacionais\n",
            "- Língua escrita\n",
            "- Língua falada\n",
            "- Línguas de sinais\n",
            "- Produção e recepção do som\n",
            "- Entonação, volume, sotaque\n",
            "- Reconhecimento de sentimentos e intenções\n",
            "- Significado de uma expressão linguística\n",
            "- Interpretação de Linguagem Natural (NLU)\n",
            "- Geração de Linguagem Natural (NLG)\n",
            "- Análise e interpretação da língua\n",
            "- Segmentação e classificação dos componentes linguísticos\n",
            "- Palavras e suas classes morfológicas e gramaticais\n",
            "- Traços semânticos ou ontológicos\n",
            "- Chatbot\n",
            "- Interação com o usuário\n",
            "- sistema\n",
            "- resposta\n",
            "- ação\n",
            "- significado\n",
            "- humano\n",
            "- aplicações\n",
            "- NLG\n",
            "- linguagem natural\n",
            "- chatbots\n",
            "- ChatGPT2\n",
            "- conceitos\n",
            "- PLN\n",
            "- ferramentas\n",
            "- recursos\n",
            "- informaçõe linguísticas\n",
            "- corretor ortográfico\n",
            "- tradutor automático\n",
            "- sumarizador automático\n",
            "- resumos\n",
            "- Usuário\n",
            "- Buscador da web\n",
            "- Passo intermediário\n",
            "- Sistema de recuperação de informação\n",
            "- Dicionário\n",
            "- Aplicativo para consulta\n",
            "- Modelo de língua\n",
            "- Chatbot\n",
            "- Conceitos\n",
            "- Caracterização\n",
            "- Exemplos\n",
            "- Aplicações\n",
            "- Processam uma entrada\n",
            "- Linguagem natural\n",
            "- Tradutor automático\n",
            "- Corretor ortográfico ou gramatical\n",
            "- Assistentes virtuais/chatbots\n",
            "- Sumarizador automático\n",
            "- Sistemas de recomendação\n",
            "- Sites de e-commerce ou entretenimento\n",
            "- Sistemas de auxílio à escrita\n",
            "- Sistemas de classificação textual\n",
            "- Sistemas de recuperação de informação\n",
            "- Sistemas de detecção de fake news\n",
            "- Recursos\n",
            "- Fontes de informação linguística\n",
            "- Léxicos\n",
            "- Dicionários monolíngues ou bilíngues\n",
            "- Corpus\n",
            "- Datasets linguísticos\n",
            "- Listas de frequências de palavras\n",
            "- Taxonomias\n",
            "- Ontologias\n",
            "- Redes de sinônimos e antônimos\n",
            "- Modelos de língua estatísticos\n",
            "- Modelos de língua neurais\n",
            "- Ferramentas\n",
            "- Segmentadores textuais\n",
            "- Stemmers\n",
            "- Lematizadores\n",
            "- Etiquetadores morfossintáticos\n",
            "- Etiquetadores semânticos\n",
            "- Analisadores sintáticos parciais\n",
            "- Analisadores sintáticos completos\n",
            "- Concordanciadores\n",
            "- Interfaces de anotação de corpus\n",
            "- Componentes em kits de ferramentas\n",
            "- NLTK3\n",
            "- Tratamento da língua\n",
            "- Português brasileiro\n",
            "- Objeto de pesquisa\n",
            "- Paradigmas do PLN\n",
            "- Capítulo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Capítulo 25**"
      ],
      "metadata": {
        "id": "92kBQ9__Cl8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correção gramatical"
      ],
      "metadata": {
        "id": "zfdPkw7lComy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aplicar_correcao_gramatical(part=10, cap=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8C_PDAhCrqf",
        "outputId": "b9a1a70f-b5e7-4b9e-e9c9-142fbdd01397"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25.1 Desafios e perspectivas para o PLN-Português\n",
            "\n",
            "Por razões históricas e econômicas, os sistemas atuais de PLN \"estado da arte\" são muito mais comuns em inglês do que em qualquer outra língua. Enquanto outras comunidades têm adaptado para suas línguas os sistemas originalmente criados para o inglês (por meio de novos treinamentos, mas com aproveitamento de parâmetros), comunidades linguísticas minoritárias e comunidades linguísticas de países menos desenvolvidos são invisibilizadas no mundo digital, com consequências negativas e diretas na sua economia e desenvolvimento.\n",
            "\n",
            "Segundo o Instituto Camões, em 2022, a comunidade de falantes de português no mundo era estimada em cerca de 260 milhões de pessoas (3,7% da população mundial), sendo o quarto idioma mais usado, depois do mandarim, inglês e espanhol. Contudo, essa representatividade não é contemplada no estado da arte da ciência, que está majoritariamente nas mãos de instituições e organizações não falantes do português. Pesquisadores brasileiros e portugueses têm levantado a necessidade de unir forças para colocar o português no lugar de destaque que ele merece.\n",
            "\n",
            "O processamento do português brasileiro tem avançado de maneira consistente desde meados da década de 1990, principalmente a partir do uso de AM e de abordagens cross-language e multilíngue, que facilitam a construção rápida de recursos e soluções e permitem a geração de uma aplicação em uma língua a partir de uma aplicação em outra língua. Mas ainda é precária a união de esforços entre os países da Comunidade de Países de Língua Portuguesa (CPLP), que inclui Portugal, Angola, Moçambique, Cabo Verde, Guiné-Bissau, São Tomé e Príncipe, além do Brasil. Se as diferenças linguísticas entre os diferentes idiomas representam barreiras para a criação de sistemas comuns, não há dúvida de que a união de esforços trará benefícios para todos.\n",
            "\n",
            "Por ora, o esforço mais visível é aquele entre os mais fortes do grupo, Brasil e Portugal, que realizam um evento científico bianual comum, o PROPOR, e mantêm vínculos acadêmicos há várias décadas. Dois grandes repositórios de recursos e ferramentas linguístico-computacionais do Português, que pretendem abranger as diversas comunidades de língua portuguesa, são a Linguateca e o Portulan Clarin.\n",
            "\n",
            "Em países extensos como o Brasil, onde há uma grande variedade linguística, a exemplo das diferentes línguas indígenas faladas em território nacional, das variações dialetais e sociais e dos sotaques regionais do português brasileiro, suas riquezas e diversidades linguísticas dificilmente são representadas nos corpora. Essa sub-representação nos dados de treinamento de modelos de aprendizado de máquina é um dos fatores que contribuem para aumentar a codificação de vieses por esses sistemas. Percebe-se, portanto, a importância de os dados linguísticos que alimentam tais sistemas serem coletados de forma adequada.\n",
            "responsável, buscando representar as variações linguísticas e idiomáticas das línguas faladas no país. Um dos primeiros corpora em português brasileiro usado para treinar um modelo de língua é o BrWac (Brazilian Portuguese Web as corpus), composto por 3,53 milhões de documentos da web, totalizando 2,68 bilhões de tokens, com acesso público para pesquisadores. Já o corpus Carolina, do Centro de IA, C4AI, é, de acordo com os autores, \"um corpus com um volume robusto de textos em Português Brasileiro contemporâneo (1970-2021), com informações de procedência e tipologia. O corpus está disponível em acesso aberto, para download gratuito, desde 8 de março de 2022. A versão atual, Ada 1.2 (8 de março de 2023), tem 823 milhões de tokens, mais de dois milhões de textos e mais de 11 GBs\". Esse corpus é um importante passo para o treinamento de LLM do português brasileiro e tem o mérito de incluir uma grande variedade de gêneros (jornalismo, literatura, poesia, judiciário, wikis, mídia social, legislativo, acadêmico etc.), mas ainda não contempla as diversidades regionais e culturais dessa língua, meta perseguida pelo C4AI com a construção do corpus de fala (transcrições) TaRSila, previsto para contemplar os diferentes dialetos brasileiros. Todos esses corpora pretendem ser variados quanto a gênero textual e domínio. No mesmo C4AI, o projeto PROINDL promete usar a IA em parceria com comunidades indígenas para o desenvolvimento de ferramentas que promovam a preservação, revitalização e disseminação de línguas indígenas do Brasil. Um dos objetivos é explorar as técnicas que utilizam poucos dados para criar tradutores automáticos tanto para texto como para fala, além de outras aplicações. Mesmo com a limitação de variedade e tamanho de corpora em português para treinamento de LLMs, grandes modelos de língua para o português já são encontrados, quer sejam modelos com capacidade multilíngue (ex. os modelos PALM da Google), quer sejam treinados apenas em português (ex. BERTimbau(Souza; Nogueira; Lotufo, 2020), Sabiá (Pires et al., 2023), Albertina). Dessa forma, são claros os avanços em direção a produtos para a língua portuguesa. No entanto, o que pode parecer simples (corpus + redes neurais e Transformers + fine-tuning = LLM) pode ser, de fato, inviável. O custo de se produzir um LLM de qualidade é extremamente alto. Um ótimo LLM, como o LLaMA-65B, por exemplo, foi pré-treinado com 1.4 trilhão de palavras, em 40 mil GPU-horas, consumindo energia equivalente ao consumo de cerca de 10 casas brasileiras em um ano. De um lado, são necessárias muitas GPUs para treinar modelos competitivos: quanto maior o número de GPUs, mais parâmetros podem ser usados no modelo, aumentando sua eficácia numa tarefa. Atualmente, poucas instituições públicas ou.\n",
            "As privadas dispõem de infraestrutura para tal e, ainda assim, com número de GPUs bastante inferior (de 2 a 100) àquela disponível em nuvem (clusters de TPUs14) com preços de aluguel que podem chegar a um milhão de dólares. Pesquisadores costumam recorrer a recursos gratuitos e temporários oferecidos pelas gigantes internacionais (ex. Google Cloud). Essa dependência externa por recursos essenciais ao desenvolvimento tecnológico só pode ser minimizada por meio de ações e investimentos governamentais (p.ex. centralizados pelo CNPq) ou por iniciativas coletivas dos detentores de recursos no sentido de juntá-los para incrementar o poder computacional e compartilhá-lo com toda a comunidade. Por outro lado, independentemente do fator financeiro, temos o custo energético, com efeito na emissão de carbono, que, como vimos, não é desprezível. Essas questões nos fazem refletir sobre os próximos caminhos a seguir. Nem tudo se resolve com grandes modelos de língua, assim como há muitas aplicações interessantes que podem ser desenvolvidas ou com modelos mais modestos ou por meios distintos dos modelos de língua. Considerando tarefas e domínios de conhecimento particulares, é possível construir soluções a partir de modelos treinados apenas nesse domínio. De fato, os resultados tendem a ser melhores do que com o uso de modelos mais genéricos. Além disso, considerar uma tarefa mais específica pode levar a uma solução - qualquer que seja a abordagem - mais eficaz. As limitações para a academia não impedem, no entanto, que o PLN seja cada vez mais usado por empresas e startups da área, cujo número vem crescendo muito em nosso país. Certamente isso é fruto da alta demanda por sistemas dessa natureza, mas também do investimento das universidades públicas na formação de recursos humanos nessa área. Estamos vivendo um momento de grande absorção dos profissionais de PLN pelo mercado. Mais um motivo para refletirmos sobre a formação desses profissionais frente aos grandes desafios que essa área (e a IA de modo geral) nos coloca. Além de todas as questões levantadas anteriormente, vale ressaltar a relevância de se adequar os critérios de avaliação tradicionalmente usados para sistemas de IA e, em particular, de PLN, à nova realidade das aplicações oferecidas à sociedade. A cultura acadêmica sugere uma avaliação em cenários rigidamente controlados, usando apenas métricas objetivas (numéricas), visando quase que exclusivamente a comparação com outros sistemas. Assim é a ciência e assim ela evolui. No entanto, tendo em vista o alcance que as novas tecnologias têm na sociedade, é urgente que os métodos de avaliação considerem critérios de outras naturezas, critérios que ajudem a prever o comportamento do sistema em situações, de fato, reais, sabidamente complexas, onde a imprevisibilidade é um fator relevante.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tradução para inglês"
      ],
      "metadata": {
        "id": "v6_caPGkCqCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aplicar_traducao_portugues_ingles(part=10, cap=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy5mWRaACsWH",
        "outputId": "7326e6f0-39d8-4c4c-f357-630dc7e9d52a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25.1 Challenges and Perspectives for Portuguese NLP\n",
            "\n",
            "For historical and economic reasons, current state-of-the-art NLP systems are much more common in English than in any other language. While other communities have adapted systems originally created for English to their own languages (through new training, but with parameter reuse), minority linguistic communities and linguistic communities in less developed countries are invisible in the digital world, with negative and direct consequences for their economy and development. According to the Camões Institute, in 2022, the Portuguese-speaking community in the world was estimated at around 260 million people (3.7% of the world's population), making it the fourth most used language, after Mandarin, English, and Spanish. However, this representation is not reflected in the state of the art of science, which is predominantly in the hands of non-Portuguese-speaking institutions and organizations. Brazilian and Portuguese researchers have raised the need to join forces to give Portuguese the prominence it deserves. The processing of Brazilian Portuguese has been consistently advancing since the mid-1990s, mainly through the use of AM and cross-language and multilingual approaches, which facilitate the rapid construction of resources and solutions and allow the generation of an application in one language from an application in another language. However, the collaboration between the countries of the Community of Portuguese-Speaking Countries (CPLP), which includes Portugal, Angola, Mozambique, Cape Verde, Guinea-Bissau, São Tomé and Príncipe, and Brazil, is still precarious. If the linguistic differences between the different languages represent barriers to the creation of common systems, there is no doubt that joint efforts will bring benefits to all. For now, the most visible effort is between the strongest members of the group, Brazil and Portugal, who hold a common biennial scientific event, PROPOR2, and have maintained academic ties for several decades. Two major repositories of linguistic-computational resources and tools for Portuguese, which aim to cover the various Portuguese-speaking communities, are Linguateca3 and Portulan Clarin4. In large countries like Brazil, where there is a great linguistic variety, such as the different indigenous languages spoken in the national territory5, dialectal and social variations, and regional accents of Brazilian Portuguese, their linguistic richness and diversity are rarely represented in corpora. This underrepresentation in the training data of machine learning models is one of the factors that contribute to the encoding of biases by these systems. Therefore, the importance of linguistics data feeding such systems being collected in a\n",
            "responsible, seeking to represent the linguistic and idiomatic variations of the languages spoken in the country. One of the first Brazilian Portuguese corpora used to train a language model is BrWac (Brazilian Portuguese Web as corpus), composed of 3.53 million web documents, totaling 2.68 billion tokens, with public access for researchers. The Carolina corpus, from the AI Center, C4AI, is, according to the authors, \"a corpus with a robust volume of texts in contemporary Brazilian Portuguese (1970-2021), with information on provenance and typology. The corpus is available for open access, for free download, since March 8, 2022. The current version, Ada 1.2 (March 8, 2023), has 823 million tokens, over two million texts, and over 11 GBs\". This corpus is an important step for training Brazilian Portuguese LLMs and has the merit of including a wide variety of genres (journalism, literature, poetry, judiciary, wikis, social media, legislative, academic, etc.), but it still does not encompass the regional and cultural diversities of this language, a goal pursued by C4AI with the construction of the speech corpus (transcriptions) TaRSila, which is intended to encompass the different Brazilian dialects. All these corpora aim to be varied in terms of textual genre and domain. In the same C4AI, the PROINDL project promises to use AI in partnership with indigenous communities for the development of tools that promote the preservation, revitalization, and dissemination of indigenous languages in Brazil. One of the objectives is to explore techniques that use limited data to create automatic translators for both text and speech, as well as other applications. Even with the limitation of variety and size of Portuguese corpora for LLM training, large language models for Portuguese are already available, whether they are multilingual models (e.g., Google's PALM models) or trained only in Portuguese (e.g., BERTimbau, Sabiá, Albertina). Thus, the advances towards products for the Portuguese language are clear. However, what may seem simple (corpus + neural networks and Transformers + fine-tuning = LLM) can be, in fact, unfeasible. The cost of producing a quality LLM is extremely high. An excellent LLM, such as LLaMA-65B, for example, was pre-trained with 1.4 trillion words, in 40,000 GPU-hours, consuming energy equivalent to the consumption of about 10 Brazilian households in one year. On one hand, many GPUs are needed to train competitive models: the more GPUs, the more parameters can be used in the model, increasing its effectiveness in a task. Currently, few public or\n",
            "Private institutions have the infrastructure for such purposes, and yet, with a significantly lower number of GPUs (ranging from 2 to 100) compared to what is available in the cloud (TPU clusters), with rental prices that can reach up to one million dollars. Researchers often rely on free and temporary resources offered by international giants (e.g. Google Cloud). This external dependence on essential resources for technological development can only be minimized through government actions and investments (e.g. centralized by CNPq) or collective initiatives by resource holders to pool them together to increase computational power and share it with the entire community. On the other hand, regardless of the financial factor, we have the energy cost, which has an effect on carbon emissions, which, as we have seen, is not negligible. These issues make us reflect on the next steps to take. Not everything is solved with large language models, just as there are many interesting applications that can be developed either with more modest models or through means other than language models. Considering specific tasks and domains of knowledge, it is possible to build solutions based solely on models trained in that domain. In fact, the results tend to be better than using more generic models. Furthermore, considering a more specific task can lead to a more effective solution, regardless of the approach. The limitations for academia, however, do not prevent Natural Language Processing (NLP) from being increasingly used by companies and startups in the field, the number of which is growing rapidly in our country. Certainly, this is the result of high demand for systems of this nature, but also of the investment made by public universities in training human resources in this area. We are experiencing a moment of great absorption of NLP professionals by the market. Another reason to reflect on the training of these professionals in the face of the great challenges that this field (and AI in general) presents to us. In addition to all the previously raised issues, it is worth highlighting the relevance of adapting the traditionally used evaluation criteria for AI systems, and particularly for NLP, to the new reality of the applications offered to society. Academic culture suggests an evaluation in rigidly controlled scenarios, using only objective (numeric) metrics, aiming almost exclusively at comparison with other systems. This is how science is and how it evolves. However, considering the reach that new technologies have in society, it is urgent that evaluation methods consider criteria of other natures, criteria that help predict the behavior of the system in real, known complex situations, where unpredictability is a relevant factor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extração das palavras chave"
      ],
      "metadata": {
        "id": "lG1pvpc-JM0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aplicar_extracao_palavras_chave(part=10, cap=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp8XwE86JNwx",
        "outputId": "faf052ee-f570-4d31-d0f1-9fd2bf0a4576"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Desafios e perspectivas para o PLN-Português\n",
            "- PLN (Processamento de Linguagem Natural)\n",
            "- Inglês\n",
            "- Línguas minoritárias\n",
            "- Línguas menos desenvolvidas\n",
            "- Economia e desenvolvimento\n",
            "- Comunidade de falantes de português\n",
            "- Instituições e organizações não falantes de português\n",
            "- Pesquisadores brasileiros e portugueses\n",
            "- União de esforços\n",
            "- Comunidade de Países de Língua Portuguesa (CPLP)\n",
            "- Diferenças linguísticas\n",
            "- Evento científico PROPOR\n",
            "- Recursos e ferramentas linguístico-computacionais\n",
            "- Linguateca\n",
            "- Portulan Clarin\n",
            "- Variedade linguística no Brasil\n",
            "- Línguas indígenas\n",
            "- Variações dialetais e sociais\n",
            "- Sotaques regionais do português brasileiro\n",
            "- Riquezas e diversidades linguísticas\n",
            "- Sub-representação nos dados de treinamento\n",
            "- Modelos de aprendizado de máquina\n",
            "- Codificação de vieses\n",
            "- Coleta de dados linguísticos\n",
            "- Responsável\n",
            "- Variações linguísticas\n",
            "- Idiomáticas\n",
            "- Línguas faladas\n",
            "- País\n",
            "- Corpora\n",
            "- Português brasileiro\n",
            "- Modelo de língua\n",
            "- BrWac\n",
            "- Brazilian Portuguese Web as corpus\n",
            "- Documentos da web\n",
            "- Tokens\n",
            "- Acesso público\n",
            "- Pesquisadores\n",
            "- Corpus Carolina\n",
            "- Centro de IA\n",
            "- C4AI\n",
            "- Volume robusto\n",
            "- Textos\n",
            "- Português Brasileiro contemporâneo\n",
            "- Informações de procedência\n",
            "- Tipologia\n",
            "- Acesso aberto\n",
            "- Download gratuito\n",
            "- Tokens\n",
            "- Textos\n",
            "- GBs\n",
            "- Treinamento de LLM\n",
            "- Gêneros\n",
            "- Jornalismo\n",
            "- Literatura\n",
            "- Poesia\n",
            "- Judiciário\n",
            "- Wikis\n",
            "- Mídia social\n",
            "- Legislativo\n",
            "- Acadêmico\n",
            "- Diversidades regionais\n",
            "- Culturais\n",
            "- Língua\n",
            "- Meta\n",
            "- Corpus de fala\n",
            "- Transcrições\n",
            "- TaRSila\n",
            "- Dialetos brasileiros\n",
            "- Projeto PROINDL\n",
            "- IA\n",
            "- Comunidades indígenas\n",
            "- Preservação\n",
            "- Revitalização\n",
            "- Disseminação\n",
            "- Línguas indígenas\n",
            "- Brasil\n",
            "- Tradutores automáticos\n",
            "- Texto\n",
            "- Fala\n",
            "- Aplicações\n",
            "- Modelos de língua\n",
            "- Capacidade multilíngue\n",
            "- PALM\n",
            "- Google\n",
            "- Treinados apenas em português\n",
            "- BERTimbau\n",
            "- Souza\n",
            "- Nogueira\n",
            "- Lotufo\n",
            "- Sabiá\n",
            "- Pires\n",
            "- Albertina\n",
            "- Avanços\n",
            "- Produtos\n",
            "- Língua portuguesa\n",
            "- Corpus\n",
            "- Redes neurais\n",
            "- Transformers\n",
            "- Fine-tuning\n",
            "- LLM\n",
            "- Custo\n",
            "- Qualidade\n",
            "- LLaMA-65B\n",
            "- Pré-treinado\n",
            "- Palavras\n",
            "- GPU\n",
            "- Horas\n",
            "- Consumo de energia\n",
            "- Casas brasileiras\n",
            "- Instituições públicas\n",
            "- Privadas\n",
            "- Infraestrutura\n",
            "- GPUs\n",
            "- Nuvem\n",
            "- TPUs\n",
            "- Aluguel\n",
            "- Milhão de dólares\n",
            "- Recursos gratuitos\n",
            "- Temporários\n",
            "- Gigantes internacionais\n",
            "- Google Cloud\n",
            "- Dependência externa\n",
            "- Desenvolvimento tecnológico\n",
            "- Ações\n",
            "- Investimentos governamentais\n",
            "- CNPq\n",
            "- Iniciativas coletivas\n",
            "- Poder computacional\n",
            "- Compartilhá-lo\n",
            "- Comunidade\n",
            "- Custo energético\n",
            "- Emissão de carbono\n",
            "- Modelos de língua\n",
            "- Aplicações interessantes\n",
            "- Modelos mais modestos\n",
            "- Domínios de conhecimento\n",
            "- Soluções\n",
            "- Abordagem\n",
            "- Eficiência\n",
            "- Limitações para a academia\n",
            "- Empresas\n",
            "- Startups\n",
            "- Universidades públicas\n",
            "- Recursos humanos\n",
            "- Mercado\n",
            "- Formação\n",
            "- Desafios\n",
            "- IA\n",
            "- Critérios de avaliação\n",
            "- Sistemas de IA\n",
            "- PLN\n",
            "- Realidade das aplicações\n",
            "- Sociedade\n",
            "- Cultura acadêmica\n",
            "- Cenários rigidamente controlados\n",
            "- Mátricas objetivas\n",
            "- Comparação\n",
            "- Novas tecnologias\n",
            "- Métodos de avaliação\n",
            "- Comportamento do sistema\n",
            "- Situações reais\n",
            "- Imprevisibilidade\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}